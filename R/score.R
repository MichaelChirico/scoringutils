#' @title Evaluate forecasts
#'
#' @description The function `score` is an easy to use wrapper
#' of the lower level functions in the \pkg{scoringutils} package.
#' It can be used to score probabilistic or quantile forecasts of
#' continuous, integer-valued or binary variables.
#'
#' @details the following metrics are used where appropriate:
#' \itemize{
#'   \item {Interval Score} for quantile forecasts. Smaller is better. See
#'   [interval_score()] for more information. By default, the
#'   weighted interval score is used.
#'   \item {Brier Score} for a probability forecast of a binary outcome.
#'   Smaller is better. See [brier_score()] for more information.
#'   \item {aem} Absolute error of the median prediction
#'   \item {Bias} 0 is good, 1 and -1 are bad.
#'   See [bias()] for more information.
#'   \item {Sharpness/dispersion} Smaller is better. See [sharpness()] for more
#'   information.
#'   \item {Calibration} represented through the p-value of the
#'   Anderson-Darling test for the uniformity of the Probability Integral
#'   Transformation (PIT). For integer valued forecasts, this p-value also
#'   has a standard deviation. Larger is better.
#'   See [pit()] for more information.
#'   \item {DSS} Dawid-Sebastiani-Score. Smaller is better.
#'   See [dss()] for more information.
#'   \item {CRPS} Continuous Ranked Probability Score. Smaller is better.
#'   See [crps()] for more information.
#'   \item {Log Score} Smaller is better. Only for continuous forecasts.
#'   See [logs()] for more information.
#' }
#'
#' @param data A data.frame or data.table with the predictions and observations.
#' Note: it is easiest to have a look at the example files provided in the
#' package and in the examples below.
#' The following columns need to be present:
#' \itemize{
#'   \item `true_value` - the true observed values
#'   \item `prediction` - predictions or predictive samples for one
#'   true value. (You only don't need to provide a prediction column if
#'   you want to score quantile forecasts in a wide range format.)}
#' For integer and continuous forecasts a `sample` column is needed:
#' \itemize{
#'   \item `sample` - an index to identify the predictive samples in the
#'   prediction column generated by one model for one true value. Only
#'   necessary for continuous and integer forecasts, not for
#'   binary predictions.}
#' For a quantile-format forecast you should provide:
#'   - `prediction`: prediction to the corresponding quantile
#'   - `quantile`: quantile to which the prediction corresponds
#' @param metrics the metrics you want to have in the output. If `NULL` (the
#' default), all available metrics will be computed. For a list of available
#' metrics see [available_metrics()]
#' @param ... additional parameters passed down to lower-level functions.
#' For example, the following arguments can change how weighted interval
#' scores are computed:
#' - `count_median_twice` that controls how the interval scores for different
#' intervals are summed up. This should be a logical (default is `FALSE`) that
#' indicates whether or not to count the median twice when summarising.
#' This would conceptually treat the
#' median as a 0% prediction interval, where the median is the lower as well as
#' the upper bound. The alternative is to treat the median as a single quantile
#' forecast instead of an interval. The interval score would then
#' be better understood as an average of quantile scores.)
#'
#' @return A data.table with appropriate scores. For binary predictions,
#' the Brier Score will be returned, for quantile predictions the interval
#' score, as well as adapted metrics for calibration, sharpness/dispersion and
#' bias. For integer forecasts, Sharpness, Bias, DSS, CRPS, LogS, and
#' pit_p_val (as an indicator of calibration) are returned. For integer
#' forecasts, pit_sd is returned (to account for the randomised PIT),
#' but no Log Score is returned (the internal estimation relies on a
#' kernel density estimate which is difficult for integer-valued forecasts).
#' If `by` is specified differently from `by`,
#' the average score per summary unit is returned.
#' If specified, quantiles and standard deviation of scores can also be returned
#' when summarising.
#'
#' @importFrom data.table ':=' as.data.table
#' @importFrom methods hasArg
#'
#' @examples
#' library("scoringutils")
#' ## Probability Forecast for Binary Target
#' eval <- score(example_binary)
#'
#' ## Quantile Forecasts
#' eval <- score(example_quantile)
#'
#' ## Integer Forecasts
#' eval <- score(example_integer)
#'
#' ## Continuous Forecasts
#' eval <- score(example_continuous)
#'
#' @author Nikos Bosse \email{nikosbosse@@gmail.com}
#' @references Funk S, Camacho A, Kucharski AJ, Lowe R, Eggo RM, Edmunds WJ
#' (2019) Assessing the performance of real-time epidemic forecasts: A
#' case study of Ebola in the Western Area region of Sierra Leone, 2014-15.
#' PLoS Comput Biol 15(2): e1006785. <doi:10.1371/journal.pcbi.1006785>
#' @export

score <- function(data,
                  metrics = NULL,
                  # compute_relative_skill = FALSE,
                  # rel_skill_metric = "auto",
                  # baseline = NULL,
                  ...) {

  # preparations ---------------------------------------------------------------
  # check relevant columns and remove NA values in true_values and prediction
  data <- check_clean_data(data, verbose = FALSE)

  # use all available metrics if none are given
  if (is.null(metrics)) {
    metrics <- available_metrics()
  }

  # obtain a value for the unit of a single observation
  forecast_unit <- get_unit_of_forecast(data)

  # # check input parameters and whether computation of relative skill is possible
  # compute_rel_skill <- check_score_params(
  #   data,
  #   forecast_unit,
  #   metrics,
  #   summarise_by,
  #   compute_relative_skill,
  #   baseline,
  #   rel_skill_metric
  # )

  # check prediction and target type -------------------------------------------
  prediction_type <- get_prediction_type(data)
  target_type <- get_target_type(data)

  # Score binary predictions ---------------------------------------------------
  if (target_type == "binary") {
    scores <- score_binary(data = data,
                           forecast_unit = forecast_unit,
                           metrics = metrics)
  }

  # Score quantile predictions -------------------------------------------------
  if (prediction_type == "quantile") {
    scores <- score_quantile(data = data,
                             forecast_unit = forecast_unit,
                             metrics = metrics,
                             ...)
  }

  # Score integer or continuous predictions ------------------------------------
  if (prediction_type %in% c("integer", "continuous") && (target_type != "binary")) {

    scores <- score_sample(data = data,
                           forecast_unit = forecast_unit,
                           metrics = metrics,
                           prediction_type = prediction_type)

    scores <- summarise_scores(scores,
                               by = forecast_unit)
  }

  return(scores[])
}
