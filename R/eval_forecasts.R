#' @title Evaluate forecasts
#'
#' @description The function \code{eval_forecasts} is an easy to use wrapper
#' of the lower level functions in the \code{scoringutils} package.
#' It can be used to score probabilistic or quantile forecasts of
#' continuous, integer-valued or binary variables.
#'
#' @details the following metrics are used where appropriate:
#' \itemize{
#'   \item {Interval Score} for quantile forecasts. Smaller is better. See
#'   \code{\link{interval_score}} for more information.
#'   \item {Brier Score} for a probability forecast of a binary outcome.
#'   Smaller is better. See \code{\link{brier_score}} for more information.
#'   \item {Bias} 0 is good, 1 and -1 are bad.
#'   See \code{\link{bias}} for more information.
#'   \item {Sharpness} Smaller is better. See \code{\link{sharpness}} for more
#'   information.
#'   \item {Calibration} represented through the p-value of the
#'   Anderson-Darling test for the uniformity of the Probability Integral
#'   Transformation (PIT). For integer valued forecasts, this p-value also
#'   has a standard deviation. Larger is better.
#'   See \code{\link{pit}} for more information.
#'   \item {DSS} Dawid-Sebastiani-Score. Smaller is better.
#'   See \code{\link{dss}} for more information.
#'   \item {CRPS} Continuous Ranked Probability Score. Smaller is better.
#'   See \code{\link{crps}} for more information.
#'   \item {Log Score} Smaller is better. Only for continuous forecasts.
#'   See \code{\link{logs}} for more information.
#' }
#'
#' @param data A data.frame or data.table with the correct columns.
#' Note: it is easiest to have a look at the example files provided in the
#' package and in the examples below.
#' Regardless of the forecast type,the following columns need to be present:
#' \itemize{
#'   \item \code{true_values} the true observed values
#'   \item \code{id} A unique identifier of the true values. Could be a date
#'   or just a running index}
#' All forecasts except the quantile forecasts need a \code{predictions} column:
#' \itemize{
#' \item \code{predictions} predictions or predictive samples for one
#'   true value.}
#' For integer and continuous forecasts a \code{sample} column is needed:
#' \itemize{
#'   \item \code{sample} an index to identify the predictive samples in the
#'   predictiions column generated by one model for one true value. Only
#'   necessary for continuous and integer forecasts, not for
#'   binary predictions.}
#' For quantile forecasts the data can either be provided in a long or in a
#' wide format. The wide format needs columns with the quantile forecasts
#' \itemize{
#'   \item {quantile forecasts} a number of pairs of columns with
#'   quantile predictions for a certain range. For a 50% interval
#'   (corresponding to the 25% and
#'   75% quantile), one column has to be named \code{lower_50} and one
#'   \code{upper_50}.
#'   For the median, there has to a column \code{lower_0} and one
#'   \code{upper_0}
#'   }
#' The long format needs the following columns
#' \itemize{
#'   \item \code{predictions} the quantile forecasts
#'   \item \code{boundary} values should be either "lower" or "upper", depending
#'   on whether the prediction is for the lower or upper bound of a given range
#'   \item {range} the range for which a forecast was made. For a 50\% interval
#'   the range should be 50. The forecast for the 25\% quantile should have
#'   the value in the \code{predictions} column, the value of \code{range}
#'   should be 50 and the value of \code{boundary} should be "lower".
#'   If you want to score the median (i.e. \code{range = 0}), you still
#'   need to include a lower and an upper estimate, so the median has to
#'   appear twice.}
#' @param by character vector of columns to group scoring by. The default
#' is \code{c("model")}, but you could e.g. group over different locations
#' or horizons. Note that a column of the corresponding name must be
#' present in the data. If you don't want any grouping, set \code{by = NULL}
#' @param summarised if \code{TRUE} (the default), only one average score is
#' returned per grouped unit.
#' @param pit_arguments pass down additional arguments to the \code{\link{pit}}
#' function.
#' @param interval_score_arguments pass down additional arguments to the
#' \code{\link{interval_score}} function, e.g. \code{weigh = TRUE}.
#'
#' @return A data.table with appropriate scores. For binary predictions,
#' the Brier Score will be returned, for quantile predictions the interval
#' score, as well as adapted metrics for calibration, sharpness and bias. The
#' calibration metric is the percentage of true values that fall into a given
#' range, the sharpness is determined as the average width of the 50\%
#' interval range and bias is estimated as the percentage of true values that
#' fall above the median, transformed to [-1, 1].
#' For integer forecasts, Sharpness, Bias, DSS, CRPS, LogS, and
#' pit_p_val (as an indicator of calibration) are returned. For integer
#' forecasts, pit_sd is returned (to account for the randomised PIT),
#' but no Log Score is returned (the internal estimation relies on a
#' kernel density estimate which is difficult for integer-valued forecasts).
#' If \code{summarised = TRUE} the average score per model is returned.
#'
#' @importFrom data.table ':=' setDT %like%
#'
#' @examples
#' ## Probability Forecast for Binary Target
#' binary_example <- data.table::setDT(scoringutils::binary_example_data)
#' eval <- scoringutils::eval_forecasts(binary_example, quantiles = c(0.7), sd = TRUE)
#' eval <- scoringutils::eval_forecasts(binary_example, summarised = FALSE)
#'
#' ## Quantile Forecasts
#' # wide format
#' quantile_example <- data.table::setDT(scoringutils::quantile_example_data_wide)
#' eval <- scoringutils::eval_forecasts(quantile_example,
#'                                      by = c("model", "horizon"),
#'                                      summarise_by = "model",
#'                                      quantiles = c(0.5),
#'                                      interval_score_arguments = list(weigh = TRUE))
#' eval <- scoringutils::eval_forecasts(quantile_example, summarised = FALSE)
#'
#' #long format
#' eval <- scoringutils::eval_forecasts(scoringutils::quantile_example_data_long)
#'
#' ## Integer Forecasts
#' integer_example <- data.table::setDT(scoringutils::integer_example_data)
#' eval <- scoringutils::eval_forecasts(integer_example,
#'                                      by = c("model", "horizon"),
#'                                      quantiles = c(0.1, 0.9),
#'                                      pit_arguments = list(n_replicates = 30,
#'                                                           plot = FALSE))
#' eval <- scoringutils::eval_forecasts(integer_example, summarised = FALSE)
#'
#' ## Continuous Forecasts
#' continuous_example <- data.table::setDT(scoringutils::continuous_example_data)
#' eval <- scoringutils::eval_forecasts(continuous_example, by = c("model", "horizon"))
#' eval <- scoringutils::eval_forecasts(continuous_example,
#'                                      by = c("model", "horizon"),
#'                                      summarised = FALSE)
#'
#' @author Nikos Bosse \email{nikosbosse@gmail.com}
#' @references Funk S, Camacho A, Kucharski AJ, Lowe R, Eggo RM, Edmunds WJ
#' (2019) Assessing the performance of real-time epidemic forecasts: A
#' case study of Ebola in the Western Area region of Sierra Leone, 2014-15.
#' PLoS Comput Biol 15(2): e1006785.
#' \url{https://doi.org/10.1371/journal.pcbi.1006785}
#' @export
#'



eval_forecasts <- function(data,
                           by = c("model"),
                           summarise_by = NULL,
                           summarised = TRUE,
                           pit_arguments = list(plot = FALSE),
                           sd = FALSE,
                           quantiles = c(),
                           interval_score_arguments = list()) {


  # preparations ---------------------------------------------------------------
  data.table::setDT(data)

  if (is.null(summarise_by)) {
    summarise_by <- by
  }


  # check if predictions are integer, continuous, etc. -------------------------
  if (any(grepl("lower", names(data))) | "boundary" %in% names(data)) {
    prediction_type <- "quantile"
  } else if (all.equal(data$predictions, as.integer(data$predictions)) == TRUE) {
    prediction_type <- "integer"
  } else {
    prediction_type <- "continuous"
  }

  if (all.equal(data$true_values, as.integer(data$true_values)) == TRUE) {
    if (all(data$true_values %in% c(0,1))) {
      target_type = "binary"
    } else {
      target_type = "integer"
    }
  } else {
    target_type = "continuous"
  }


  # Score binary predictions ---------------------------------------------------
  if (target_type == "binary") {

    res <- data[, "brier_score" := scoringutils::brier_score(true_values, predictions),
         by = c("id", by)]

    if (summarised) {
      # add quantiles
      if (!is.null(quantiles)) {
        res[, paste0("brier_score_", quantiles) := as.list(quantile(brier_score,
                                                                       probs = quantiles,
                                                                       na.rm = TRUE)),
            by = c(summarise_by)]
      }

      # add standard deviation
      if (sd) {
        res[, "brier_score_sd" := sd(brier_score, na.rm = TRUE), by = c(summarise_by)]
      }

      # summarise by taking the mean over all relevant columns
      res <- data[, lapply(.SD, mean, na.rm = TRUE),
                 .SDcols = colnames(res) %like% "brier",
                 by = summarise_by]

    }
    return(res)
  }

  # Score quantile predictions -------------------------------------------------
  if (prediction_type == "quantile") {

    # check if long or wide format
    if ("boundary" %in% names(data)) {
      wide = FALSE
    } else {
      wide = TRUE
    }

    if (wide) {
      # convert into long format
      colnames <- colnames(data)
      ranges <- colnames[grepl("lower", colnames) | grepl("upper", colnames)]

      data <- data.table::melt(data,
                               measure.vars = ranges,
                               variable.name = "range",
                               value.name = "predictions")
      data[, boundary := gsub("_.*", "", range)]
      data[, range := as.numeric(gsub("^.*?_","", range))]
    }

    data <- data.table::dcast(data, ... ~ boundary,
                              value.var = "predictions")
    res <- data[, "interval_score" := do.call(scoringutils::interval_score,
                                              c(list(true_values,
                                                     lower,
                                                     upper,
                                                     range),
                                                interval_score_arguments))]

    # compute calibration
    res[, calibration := mean(true_values >= lower & true_values <= upper),
        by = c("range", by)]

    # compute bias as fraction of true_values above the median and transformed to [-1, 1]
    # only possible if median forecast exists
    if (0 %in% unique(res$range)) {
      bias <- res[range == 0,
                  .(bias = 1 - 2 * mean(true_values > lower)),
                  by = by]

      res <-  merge(res, bias, by = by)
    }


    # compute sharpness as average width of the 50% interval
    if (50 %in% unique(res$range)) {
      sharpness <- res[range == 50, .(sharpness = mean(upper - lower)),
                       by = by]
      res <-  merge(res, sharpness, by = by)
    }

    if (summarised) {

      if (!is.null(quantiles)) {
        # add quantiles for the scores
        res[, paste0("interval_score_", quantiles) := as.list(quantile(interval_score,
                                                                       probs = quantiles,
                                                                       na.rm = TRUE)),
            by = c(summarise_by, "range")]

        res[, paste0("calibration_", quantiles) := as.list(quantile(calibration,
                                                                    probs = quantiles,
                                                                    na.rm = TRUE)),
            by = c(summarise_by, "range")]

        res[, paste0("bias_", quantiles) := as.list(quantile(bias,
                                                             probs = quantiles,
                                                             na.rm = TRUE)),
            by = c(summarise_by, "range")]

        res[, paste0("sharpness_", quantiles) := as.list(quantile(sharpness,
                                                                  probs = quantiles,
                                                                  na.rm = TRUE)),
            by = c(summarise_by, "range")]
      }

      # add standard deviation
      if (sd) {
        res[, "interval_score_sd" := sd(interval_score, na.rm = TRUE), by = c(summarise_by, "range")]
        res[, "bias_sd" := sd(bias, na.rm = TRUE), by = c(summarise_by, "range")]
        res[, "calibration_sd" := sd(calibration, na.rm = TRUE), by = c(summarise_by, "range")]
        res[, "sharpness_sd" := sd(sharpness, na.rm = TRUE), by = c(summarise_by, "range")]
      }

      # summarise by taking the mean and omitting unecessary columns
      res <- res[, lapply(.SD, mean, na.rm = TRUE),
                 by = c(summarise_by, "range"),
                 .SDcols = colnames(res) %like% "calibration|bias|sharpness|interval_score"]
    }
    return(res)
  }


  # Score integer or continuous predictions ------------------------------------
  # sharpness
  data[, sharpness := scoringutils::sharpness(t(predictions)), by = c("id", by)]

  # bias
  data[, bias := scoringutils::bias(unique(true_values),
                                     t(predictions)), by = c("id", by)]

  # DSS
  data[, dss := scoringutils::dss(unique(true_values),
                                    t(predictions)), by = c("id", by)]

  # CRPS
  data[, crps := scoringutils::crps(unique(true_values),
                                    t(predictions)), by = c("id", by)]

  # Log Score
  if (prediction_type == "continuous") {
    data[, log_score := scoringutils::logs(unique(true_values),
                                       t(predictions)), by = c("id", by)]
  }

  # calibration
  # reformat data.table to wide format
  dat <- data.table::dcast(data, ... ~ paste("sampl_", sample, sep = ""),
                           value.var = "predictions")

  # compute pit p-values
  dat[, c("pit_p_val", "pit_sd") := do.call(pit, c(list(true_values,
                                                        as.matrix(.SD)),
                                                   pit_arguments)),
      .SDcols = names(dat)[grepl("sampl_", names(dat))], by = by]

  # remove variables not necessary for merging
  dat[, names(dat)[grepl("sampl_", names(dat))] := NULL]
  dat[, c("sharpness", "bias", "dss", "crps") := NULL]

  # merge with previous data
  merge_cols = colnames(dat)[!colnames(dat) %in% c("pit_p_val", "pit_sd")]
  res <- merge(data, dat, by = merge_cols)

  # @ SAM
  # scores are computed over 'by'. This means that scores are duplicated.
  # we could either leave it unsummarised - or filter out all duplicates, so that only
  # unique scores remain. Do we want that?
  # previous version was mean, na.rm = TRUE, but unqique should do the same
  res <- res[, lapply(.SD, unique),
             .SDcols = colnames(res) %like% "pit_|bias|sharpness|dss|crps|log_score",
             by = c("id", by)]


  if (summarised) {
    # add quantiles
    if (!is.null(quantiles)) {
      res[, paste0("crps_", quantiles) := as.list(quantile(crps,
                                                           probs = quantiles,
                                                           na.rm = TRUE)),
          by = summarise_by]

      res[, paste0("dss_", quantiles) := as.list(quantile(dss,
                                                           probs = quantiles,
                                                           na.rm = TRUE)),
          by = by]

      if (prediction_type == "continuous") {
        res[, paste0("log_score_", quantiles) := as.list(quantile(log_score,
                                                             probs = quantiles,
                                                             na.rm = TRUE)),
            by = summarise_by]
      }

      res[, paste0("bias_", quantiles) := as.list(quantile(bias,
                                                           probs = quantiles,
                                                           na.rm = TRUE)),
          by = by]

      res[, paste0("pit_p_val_", quantiles) := as.list(quantile(pit_p_val,
                                                           probs = quantiles,
                                                           na.rm = TRUE)),
          by = summarise_by]

      res[, paste0("sharpness_", quantiles) := as.list(quantile(sharpness,
                                                           probs = quantiles,
                                                           na.rm = TRUE)),
          by = summarise_by]
    }

    if (sd) {
      # add standard deviations
      res[, "crps_sd" := sd(crps, na.rm = TRUE), by = c(summarise_by)]
      res[, "dss_sd" := sd(crps, na.rm = TRUE), by = c(summarise_by)]
      if (prediction_type == "continuous") {
        res[, "log_score_sd" := sd(log_score, na.rm = TRUE), by = c(summarise_by)]
      }
      res[, "bias_sd" := sd(bias, na.rm = TRUE), by = c(summarise_by)]
      res[, "sharpness_sd" := sd(sharpness, na.rm = TRUE), by = c(summarise_by)]
    }


    res <- res[, lapply(.SD, mean, na.rm = TRUE),
               .SDcols = colnames(res) %like% "pit_|bias|sharpness|dss|crps|log_score",
               by = summarise_by]
  }
  return (res)
}










