#' @title Evaluate forecasts
#'
#' @description The function `eval_forecasts` is an easy to use wrapper
#' of the lower level functions in the \pkg{scoringutils} package.
#' It can be used to score probabilistic or quantile forecasts of
#' continuous, integer-valued or binary variables.
#'
#' @details the following metrics are used where appropriate:
#' \itemize{
#'   \item {Interval Score} for quantile forecasts. Smaller is better. See
#'   [interval_score()] for more information. By default, the
#'   weighted interval score is used.
#'   \item {Brier Score} for a probability forecast of a binary outcome.
#'   Smaller is better. See [brier_score()] for more information.
#'   \item {aem} Absolute error of the median prediction
#'   \item {Bias} 0 is good, 1 and -1 are bad.
#'   See [bias()] for more information.
#'   \item {Sharpness/dispersion} Smaller is better. See [sharpness()] for more
#'   information.
#'   \item {Calibration} represented through the p-value of the
#'   Anderson-Darling test for the uniformity of the Probability Integral
#'   Transformation (PIT). For integer valued forecasts, this p-value also
#'   has a standard deviation. Larger is better.
#'   See [pit()] for more information.
#'   \item {DSS} Dawid-Sebastiani-Score. Smaller is better.
#'   See [dss()] for more information.
#'   \item {CRPS} Continuous Ranked Probability Score. Smaller is better.
#'   See [crps()] for more information.
#'   \item {Log Score} Smaller is better. Only for continuous forecasts.
#'   See [logs()] for more information.
#' }
#'
#' @param data A data.frame or data.table with the predictions and observations.
#' Note: it is easiest to have a look at the example files provided in the
#' package and in the examples below.
#' The following columns need to be present:
#' \itemize{
#'   \item `true_value` - the true observed values
#'   \item `prediction` - predictions or predictive samples for one
#'   true value. (You only don't need to provide a prediction column if
#'   you want to score quantile forecasts in a wide range format.)}
#' For integer and continuous forecasts a `sample` column is needed:
#' \itemize{
#'   \item `sample` - an index to identify the predictive samples in the
#'   prediction column generated by one model for one true value. Only
#'   necessary for continuous and integer forecasts, not for
#'   binary predictions.}
#' For quantile forecasts the data can be provided in variety of formats. You
#' can either use a range-based format or a quantile-based format. (You can
#' convert between formats using [quantile_to_range_long()],
#' [range_long_to_quantile()],
#' [sample_to_range_long()],
#' [sample_to_quantile()])
#' For a quantile-format forecast you should provide:
#'   - `prediction`: prediction to the corresponding quantile
#'   - `quantile`: quantile to which the prediction corresponds
#' For a range format (long) forecast you need
#'   - `prediction`: the quantile forecasts
#'   - `boundary`: values should be either "lower" or "upper", depending
#'   on whether the prediction is for the lower or upper bound of a given range
#'   - `range` the range for which a forecast was made. For a 50% interval
#'   the range should be 50. The forecast for the 25% quantile should have
#'   the value in the `prediction` column, the value of `range`
#'   should be 50 and the value of `boundary` should be "lower".
#'   If you want to score the median (i.e. `range = 0`), you still
#'   need to include a lower and an upper estimate, so the median has to
#'   appear twice.
#' Alternatively you can also provide the format in a wide range format.
#' This format needs:
#'   - pairs of columns called something like 'upper_90' and 'lower_90',
#'   or 'upper_50' and 'lower_50', where the number denotes the interval range.
#'   For the median, you need to provide columns called 'upper_0' and 'lower_0'
#' @param summarise_by character vector of columns to group the summary by. By
#' default, this is equal to `by` and no summary takes place.
#' But sometimes you may want to to summarise
#' over categories different from the scoring.
#' `summarise_by` is also the grouping level used to compute
#' (and possibly plot) the probability integral transform(pit). Sometimes you
#' may want to include 'range', 'quantile' or 'sample', to summarise by
#' range, quantile or sample.
#' @param metrics the metrics you want to have in the output. If `NULL` (the
#' default), all available metrics will be computed. For a list of available
#' metrics see [available_metrics()]
#' @param quantiles numeric vector of quantiles to be returned when summarising.
#' Instead of just returning a mean, quantiles will be returned for the
#' groups specified through `summarise_by`. By default, no quantiles are
#' returned.
#' @param sd if `TRUE` (the default is `FALSE`) the standard deviation of all
#' metrics will be returned when summarising.
#' @param compute_relative_skill logical, whether or not to compute relative
#' performance between models. If `TRUE` (default is `FALSE`), then a column called
#' 'model' must be present in the input data. For more information on
#' the computation of relative skill, see [pairwise_comparison()].
#' Relative skill will be calculated for the aggregation level specified in
#' `summarise_by`.
#' @param rel_skill_metric character string with the name of the metric for which
#' a relative skill shall be computed. If equal to 'auto' (the default), then
#' one of interval score, crps or brier score will be used where appropriate
#' @param baseline character string with the name of a model. If a baseline is
#' given, then a scaled relative skill with respect to the baseline will be
#' returned. By default (`NULL`), relative skill will not be scaled with
#' respect to a baseline model.
#' @param ... additional parameters passed down to lower-level functions.
#' For example, the following arguments can change how weighted interval
#' scores are computed:
#' - `count_median_twice` that controls how the interval scores for different
#' intervals are summed up. This should be a logical (default is `FALSE`) that
#' indicates whether or not to count the median twice when summarising.
#' This would conceptually treat the
#' median as a 0% prediction interval, where the median is the lower as well as
#' the upper bound. The alternative is to treat the median as a single quantile
#' forecast instead of an interval. The interval score would then
#' be better understood as an average of quantile scores.)
#'
#' @return A data.table with appropriate scores. For binary predictions,
#' the Brier Score will be returned, for quantile predictions the interval
#' score, as well as adapted metrics for calibration, sharpness/dispersion and
#' bias. For integer forecasts, Sharpness, Bias, DSS, CRPS, LogS, and
#' pit_p_val (as an indicator of calibration) are returned. For integer
#' forecasts, pit_sd is returned (to account for the randomised PIT),
#' but no Log Score is returned (the internal estimation relies on a
#' kernel density estimate which is difficult for integer-valued forecasts).
#' If `summarise_by` is specified differently from `by`,
#' the average score per summary unit is returned.
#' If specified, quantiles and standard deviation of scores can also be returned
#' when summarising.
#'
#' @importFrom data.table ':=' as.data.table
#' @importFrom methods hasArg
#'
#' @examples
#' ## Probability Forecast for Binary Target
#' binary_example <- data.table::setDT(scoringutils::binary_example_data)
#' eval <- scoringutils::eval_forecasts(binary_example,
#'                                      summarise_by = c("model"),
#'                                      quantiles = c(0.5), sd = TRUE)
#'
#' ## Quantile Forecasts
#' # wide format example (this examples shows usage of both wide formats)
#' range_example_wide <- data.table::setDT(scoringutils::range_example_data_wide)
#' range_example <- scoringutils::range_wide_to_long(range_example_wide)
#' wide2 <- data.table::setDT(scoringutils::range_example_data_semi_wide)
#' range_example <- scoringutils::range_wide_to_long(wide2)
#' example <- scoringutils::range_long_to_quantile(range_example)
#' eval <- scoringutils::eval_forecasts(example,
#'                                      summarise_by = "model",
#'                                      quantiles = c(0.05, 0.95),
#'                                      sd = TRUE)
#' eval <- scoringutils::eval_forecasts(example)
#'
#'
#' ## Integer Forecasts
#' integer_example <- data.table::setDT(scoringutils::integer_example_data)
#' eval <- scoringutils::eval_forecasts(integer_example,
#'                                      summarise_by = c("model"),
#'                                      quantiles = c(0.1, 0.9),
#'                                      sd = TRUE)
#' eval <- scoringutils::eval_forecasts(integer_example)
#'
#' ## Continuous Forecasts
#' continuous_example <- data.table::setDT(scoringutils::continuous_example_data)
#' eval <- scoringutils::eval_forecasts(continuous_example)
#' eval <- scoringutils::eval_forecasts(continuous_example,
#'                                      quantiles = c(0.5, 0.9),
#'                                      sd = TRUE,
#'                                      summarise_by = c("model"))
#'
#' @author Nikos Bosse \email{nikosbosse@@gmail.com}
#' @references Funk S, Camacho A, Kucharski AJ, Lowe R, Eggo RM, Edmunds WJ
#' (2019) Assessing the performance of real-time epidemic forecasts: A
#' case study of Ebola in the Western Area region of Sierra Leone, 2014-15.
#' PLoS Comput Biol 15(2): e1006785. <doi:10.1371/journal.pcbi.1006785>
#' @export

eval_forecasts <- function(data,
                           summarise_by = NULL,
                           metrics = NULL,
                           quantiles = c(),
                           sd = FALSE,
                           compute_relative_skill = FALSE,
                           rel_skill_metric = "auto",
                           baseline = NULL,
                           ...) {


  # preparations ---------------------------------------------------------------
  # check relevant columns and remove NA values in true_values and prediction
  data <- check_clean_data(data, verbose = FALSE)

  # error handling for relative skill computation
  if (compute_relative_skill) {
    if (!("model" %in% colnames(data))) {
      warning("to compute relative skills, there must column present called 'model'. Relative skill will not be computed")
      compute_relative_skill <- FALSE
    }
    models <- unique(data$model)
    if (length(models) < 2 + (!is.null(baseline))) {
      warning("you need more than one model non-baseline model to make model comparisons. Relative skill will not be computed")
      compute_relative_skill <- FALSE
    }
    if (!is.null(baseline) && !(baseline %in% models)) {
      warning("The baseline you provided for the relative skill is not one of the models in the data. Relative skill will not be computed")
      compute_relative_skill <- FALSE
    }
    if (rel_skill_metric != "auto" && !(rel_skill_metric %in% available_metrics())) {
      warning("argument 'rel_skill_metric' must either be 'auto' or one of the metrics that can be computed. Relative skill will not be computed")
      compute_relative_skill <- FALSE
    }
  }

  # obtain a value for the unit of a single observation
  forecast_unit <- get_unit_of_forecast(data)

  if (is.null(summarise_by)) {
    summarise_by <- forecast_unit
  }

  # check that the arguments in by and summarise_by are actually present
  if (!all(c(forecast_unit, summarise_by) %in% c(colnames(data), "range", "quantile"))) {
    not_present <- setdiff(unique(c(forecast_unit, summarise_by)),
                           c(colnames(data), "range", "quantile"))
    msg <- paste0("The following items in `summarise_by` are not",
                 "valid column names of the data: '",
                 paste(not_present, collapse = ", "),
                 "'. Check and run `eval_forecasts()` again")
    stop(msg)
  }

  # check metrics to be computed
  available_metrics <- available_metrics()
  if (is.null(metrics)) {
    metrics <- available_metrics
  } else {
    if (!all(metrics %in% available_metrics)) {
      msg <- paste("The following metrics are not currently implemented and",
                     "will not be computed:",
                     paste(setdiff(metrics, available_metrics), collapse = ", "))
      warning(msg)
    }
  }


  # check prediction and target type -------------------------------------------
  if (any(grepl("lower", names(data))) | "boundary" %in% names(data) |
      "quantile" %in% names(data) | "range" %in% names(data)) {
    prediction_type <- "quantile"
  } else if (isTRUE(all.equal(data$prediction, as.integer(data$prediction)))) {
    prediction_type <- "integer"
  } else {
    prediction_type <- "continuous"
  }

  if (isTRUE(all.equal(data$true_value, as.integer(data$true_value)))) {
    if (all(data$true_value %in% c(0,1)) && all(data$prediction >= 0) && all(data$prediction <= 1)) {
      target_type = "binary"
    } else {
      target_type = "integer"
    }
  } else {
    target_type = "continuous"
  }

  # Score binary predictions ---------------------------------------------------
  if (target_type == "binary") {
    res <- eval_forecasts_binary(data = data,
                                 forecast_unit = forecast_unit,
                                 summarise_by = summarise_by,
                                 metrics = metrics,
                                 quantiles = quantiles,
                                 sd = sd)
    return(res)
  }

  # Score quantile predictions -------------------------------------------------
  if (prediction_type == "quantile") {
    res <- eval_forecasts_quantile(data = data,
                                   forecast_unit = forecast_unit,
                                   summarise_by = summarise_by,
                                   metrics = metrics,
                                   quantiles = quantiles,
                                   sd = sd,
                                   compute_relative_skill = compute_relative_skill,
                                   rel_skill_metric = rel_skill_metric,
                                   baseline = baseline,
                                   ...)
    return(res)
  }


  # Score integer or continuous predictions ------------------------------------
  if (prediction_type %in% c("integer", "continuous")) {

    # compute scores -----------------------------------------------------------
    res <- eval_forecasts_sample(data = data,
                                 forecast_unit = forecast_unit,
                                 summarise_by = summarise_by,
                                 metrics = metrics,
                                 prediction_type = prediction_type,
                                 quantiles = quantiles,
                                 sd = sd)
    return(res)
  }
}













