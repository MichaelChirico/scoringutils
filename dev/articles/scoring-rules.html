<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="scoringutils">
<title>Scoring rules in `scoringutils` • scoringutils</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Scoring rules in `scoringutils`">
<meta property="og:description" content="scoringutils">
<meta name="robots" content="noindex">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light" data-bs-theme="light"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">scoringutils</a>

    <small class="nav-text text-danger me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="In-development version">1.2.2.9000</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../articles/scoringutils.html">Get started</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="active nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <a class="dropdown-item" href="../articles/Deprecated-functions.html">Deprecated functions</a>
    <a class="dropdown-item" href="../articles/Deprecated-visualisations.html">Deprecated Visualisations</a>
    <a class="dropdown-item" href="../articles/scoring-rules.html">Scoring rules in `scoringutils`</a>
  </div>
</li>
<li class="nav-item">
  <a class="nav-link" href="../news/index.html">Changelog</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/epiforecasts/scoringutils/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Scoring rules in `scoringutils`</h1>
                        <h4 data-toc-skip class="author">Nikos
Bosse</h4>
            
            <h4 data-toc-skip class="date">2024-04-15</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/epiforecasts/scoringutils/blob/HEAD/vignettes/scoring-rules.Rmd" class="external-link"><code>vignettes/scoring-rules.Rmd</code></a></small>
      <div class="d-none name"><code>scoring-rules.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>This vignette gives an overview of the default scoring rules made
available through the <code>scoringutils</code> package. You can, of
course, also use your own scoring rules, provided they follow the same
format. If you want to obtain more detailed information about how the
pacakge works, have a look at the <a href="https://drive.google.com/file/d/1URaMsXmHJ1twpLpMl1sl2HW4lPuUycoj/view?usp=drive_link" class="external-link">revised
version</a> of our <code>scoringutils</code> paper.</p>
<p>We can distinguish two types of forecasts: point forecasts and
probabilistic forecasts. A point forecast is a single number
representing a single outcome. A probabilistic forecast is a full
predictive probability distribution over multiple possible outcomes. In
contrast to point forecasts, probabilistic forecasts incorporate
uncertainty about different possible outcomes.</p>
<p>Scoring rules are functions that take a forecast and an observation
as input and return a single numeric value. For point forecasts, they
take the form <span class="math inline">\(S(\hat{y}, y)\)</span>, where
<span class="math inline">\(\hat{y}\)</span> is the forecast and <span class="math inline">\(y\)</span> is the observation. For probabilistic
forecasts, they usually take the form <span class="math inline">\(S(F,
y)\)</span>, where <span class="math inline">\(F\)</span> is the
cumulative density function (CDF) of the predictive distribution and
<span class="math inline">\(y\)</span> is the observation. By
convention, scoring rules are usually negatively oriented, meaning that
smaller values are better (the best possible score is usually zero). In
that sense, the score can be understood as a penalty.</p>
<p>Many scoring rules for probabilistic forecasts are so-called
(strictly) proper scoring rules. Essentially, this means that they
cannot be “cheated”: A forecaster evaluated by a strictly proper scoring
rule is always incentivised to report her honest best belief about the
future and cannot, in expectation, improve her score by reporting
something else. A more formal definition is the following: Let <span class="math inline">\(G\)</span> be the true, unobserved data-generating
distribution. A scoring rule is said to be proper, if under <span class="math inline">\(G\)</span> and for an ideal forecast <span class="math inline">\(F = G\)</span>, there is no forecast <span class="math inline">\(F' \neq F\)</span> that in expectation
receives a better score than <span class="math inline">\(F\)</span>. A
scoring rule is considered strictly proper if, under <span class="math inline">\(G\)</span>, no other forecast <span class="math inline">\(F'\)</span> in expectation receives a score
that is better than or the same as that of <span class="math inline">\(F\)</span>.</p>
<hr>
</div>
<div class="section level2">
<h2 id="metrics-for-point-forecasts">Metrics for point forecasts<a class="anchor" aria-label="anchor" href="#metrics-for-point-forecasts"></a>
</h2>
<p>See a list of the default metrics for point forecasts by calling
<code>?metrics_point()</code>.</p>
<p>This is an overview of the input and output formats for point
forecasts:</p>
<div class="figure">
<img src="scoring-rules/input-point.png" alt="Input and output formats: metrics for point." width="100%"><p class="caption">
Input and output formats: metrics for point.
</p>
</div>
<div class="section level3">
<h3 id="a-note-of-caution">A note of caution<a class="anchor" aria-label="anchor" href="#a-note-of-caution"></a>
</h3>
<p>Scoring point forecasts can be tricky business. Depending on the
choice of the scoring rule, a forecaster who is clearly worse than
another, might consistently receive better scores (see <span class="citation">Gneiting (2011)</span> for an illustrative
example).</p>
<p>Every scoring rule for a point forecast is implicitly minimised by a
specific aspect of the predictive distribution. The mean squared error,
for example, is only a meaningful scoring rule if the forecaster
actually reported the mean of their predictive distribution as a point
forecast. If the forecaster reported the median, then the mean absolute
error would be the appropriate scoring rule. If the scoring rule and the
predictive task do not align, misleading results ensue. Consider the
following example:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">1000</span></span>
<span><span class="va">observed</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">5</span>, <span class="fl">4</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span></span>
<span></span>
<span><span class="va">predicted_mu</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="va">observed</span><span class="op">)</span></span>
<span><span class="va">predicted_not_mu</span> <span class="op">&lt;-</span> <span class="va">predicted_mu</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">10</span>, <span class="fl">2</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="fu">Metrics</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/Metrics/man/ae.html" class="external-link">ae</a></span><span class="op">(</span><span class="va">observed</span>, <span class="va">predicted_mu</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 34.45981</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="fu">Metrics</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/Metrics/man/ae.html" class="external-link">ae</a></span><span class="op">(</span><span class="va">observed</span>, <span class="va">predicted_not_mu</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 32.54821</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="fu">Metrics</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/Metrics/man/se.html" class="external-link">se</a></span><span class="op">(</span><span class="va">observed</span>, <span class="va">predicted_mu</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 2171.089</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="fu">Metrics</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/Metrics/man/se.html" class="external-link">se</a></span><span class="op">(</span><span class="va">observed</span>, <span class="va">predicted_not_mu</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 2290.155</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="absolute-error">Absolute error<a class="anchor" aria-label="anchor" href="#absolute-error"></a>
</h3>
<p><strong>Observation</strong>: <span class="math inline">\(y\)</span>,
a real number</p>
<p><strong>Forecast</strong>: <span class="math inline">\(\hat{y}\)</span>, a real number, the median of the
forecaster’s predictive distribution.</p>
<p>The absolute error is the absolute difference between the predicted
and the observed values. See <code><a href="https://rdrr.io/pkg/Metrics/man/ae.html" class="external-link">?Metrics::ae</a></code>.</p>
<p><span class="math display">\[\text{ae} = |y - \hat{y}|\]</span></p>
<p>The absolute error is only an appropriate rule if <span class="math inline">\(\hat{y}\)</span> corresponds to the median of the
forecaster’s predictive distribution. Otherwise, results will be
misleading (see <span class="citation">Gneiting (2011)</span>).</p>
</div>
<div class="section level3">
<h3 id="squared-error">Squared error<a class="anchor" aria-label="anchor" href="#squared-error"></a>
</h3>
<p><strong>Observation</strong>: <span class="math inline">\(y\)</span>,
a real number</p>
<p><strong>Forecast</strong>: <span class="math inline">\(\hat{y}\)</span>, a real number, the mean of the
forecaster’s predictive distribution.</p>
<p>The squared error is the squared difference between the predicted and
the observed values. See <code><a href="https://rdrr.io/pkg/Metrics/man/se.html" class="external-link">?Metrics::se</a></code>.</p>
<p><span class="math display">\[\text{se} = (y - \hat{y})^2\]</span> The
squared error is only an appropriate rule if <span class="math inline">\(\hat{y}\)</span> corresponds to the mean of the
forecaster’s predictive distribution. Otherwise, results will be
misleading (see <span class="citation">Gneiting (2011)</span>).</p>
</div>
<div class="section level3">
<h3 id="absolute-percentage-error">Absolute percentage error<a class="anchor" aria-label="anchor" href="#absolute-percentage-error"></a>
</h3>
<p><strong>Observation</strong>: <span class="math inline">\(y\)</span>,
a real number</p>
<p><strong>Forecast</strong>: <span class="math inline">\(\hat{y}\)</span>, a real number</p>
<p>The absolute percentage error is the absolute percent difference
between the predicted and the observed values. See
<code><a href="https://rdrr.io/pkg/Metrics/man/ape.html" class="external-link">?Metrics::ape</a></code>.</p>
<p><span class="math display">\[\text{ape} = \frac{|y -
\hat{y}|}{|y|}\]</span></p>
<p>The absolute percentage error is only an appropriate rule if <span class="math inline">\(\hat{y}\)</span> corresponds to the <span class="math inline">\(\beta\)</span>-median of the forecaster’s
predictive distribution with <span class="math inline">\(\beta =
-1\)</span>. The <span class="math inline">\(\beta\)</span>-median,
<span class="math inline">\(\text{med}^{(\beta)}(F)\)</span>, is the
median of a random variable whose density is proportional to <span class="math inline">\(y^\beta f(y)\)</span>. The specific <span class="math inline">\(\beta\)</span>-median that corresponds to the
absolute percentage error is <span class="math inline">\(\text{med}^{(-1)}(F)\)</span>. Otherwise, results
will be misleading (see <span class="citation">Gneiting
(2011)</span>).</p>
<hr>
</div>
</div>
<div class="section level2">
<h2 id="binary-forecasts">Binary forecasts<a class="anchor" aria-label="anchor" href="#binary-forecasts"></a>
</h2>
<p>See a list of the default metrics for point forecasts by calling
<code>?metrics_binary()</code>.</p>
<p>This is an overview of the input and output formats for point
forecasts:</p>
<div class="figure">
<img src="scoring-rules/input-binary.png" alt="Input and output formats: metrics for binary forecasts." width="100%"><p class="caption">
Input and output formats: metrics for binary forecasts.
</p>
</div>
<div class="section level3">
<h3 id="brier-score">Brier score<a class="anchor" aria-label="anchor" href="#brier-score"></a>
</h3>
<p><strong>Observation</strong>: <span class="math inline">\(y\)</span>,
either 0 or 1</p>
<p><strong>Forecast</strong>: <span class="math inline">\(p\)</span>, a
probability that the observed outcome will be 1.</p>
<p>The Brier score is a strictly proper scoring rule. It is computed as
the mean squared error between the probabilistic prediction and the
observed outcome.</p>
<p><span class="math display">\[\begin{equation}
    \text{BS}(p, y) = (p - y)^2 =
    \begin{cases}
        p^2,       &amp; \text{if } y = 1\\
        (1 - p)^2,   &amp; \text{if } y = 0
    \end{cases}
\end{equation}\]</span></p>
<p>The Brier score and the logarithmic score (see below) differ in how
they penalise over- and underconfidence (see <span class="citation">Machete (2012)</span>). The Brier score penalises
overconfidence and underconfidence in probability space the same.
Consider the following example:</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">1e6</span></span>
<span><span class="va">p_true</span> <span class="op">&lt;-</span> <span class="fl">0.7</span></span>
<span><span class="va">observed</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html" class="external-link">factor</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html" class="external-link">rbinom</a></span><span class="op">(</span>n <span class="op">=</span> <span class="va">n</span>, size <span class="op">=</span> <span class="fl">1</span>, prob <span class="op">=</span> <span class="va">p_true</span><span class="op">)</span>, levels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">p_over</span> <span class="op">&lt;-</span> <span class="va">p_true</span> <span class="op">+</span> <span class="fl">0.15</span></span>
<span><span class="va">p_under</span> <span class="op">&lt;-</span> <span class="va">p_true</span> <span class="op">-</span> <span class="fl">0.15</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">abs</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="fu"><a href="../reference/scoring-functions-binary.html">brier_score</a></span><span class="op">(</span><span class="va">observed</span>, <span class="va">p_true</span><span class="op">)</span><span class="op">)</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="fu"><a href="../reference/scoring-functions-binary.html">brier_score</a></span><span class="op">(</span><span class="va">observed</span>, <span class="va">p_over</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.0223866</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">abs</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="fu"><a href="../reference/scoring-functions-binary.html">brier_score</a></span><span class="op">(</span><span class="va">observed</span>, <span class="va">p_true</span><span class="op">)</span><span class="op">)</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="fu"><a href="../reference/scoring-functions-binary.html">brier_score</a></span><span class="op">(</span><span class="va">observed</span>, <span class="va">p_under</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.0226134</span></span></code></pre></div>
<p>See <code>?brier_score()</code> for more information.</p>
</div>
<div class="section level3">
<h3 id="logarithmic-score">Logarithmic score<a class="anchor" aria-label="anchor" href="#logarithmic-score"></a>
</h3>
<p><strong>Observation</strong>: <span class="math inline">\(y\)</span>,
either 0 or 1</p>
<p><strong>Forecast</strong>: <span class="math inline">\(p\)</span>, a
probability that the observed outcome will be 1.</p>
<p>The logarithmic score (or log score) is a strictly proper scoring
rule. It is computed as the negative logarithm of the probability
assigned to the observed outcome.</p>
<p><span class="math display">\[\begin{equation}
    \text{Log score}(p, y) = - \log(1 - |y - p|) =
    \begin{cases}
        -\log (p),       &amp; \text{if } y = 1\\
        -\log (1 - p),   &amp; \text{if } y = 0
    \end{cases}
\end{equation}\]</span></p>
<p>The log score penalises overconfidence more strongly than
underconfidence (in probability space). Consider the following
example:</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">abs</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="fu"><a href="../reference/scoring-functions-binary.html">logs_binary</a></span><span class="op">(</span><span class="va">observed</span>, <span class="va">p_true</span><span class="op">)</span><span class="op">)</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="fu"><a href="../reference/scoring-functions-binary.html">logs_binary</a></span><span class="op">(</span><span class="va">observed</span>, <span class="va">p_over</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.07169954</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">abs</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="fu"><a href="../reference/scoring-functions-binary.html">logs_binary</a></span><span class="op">(</span><span class="va">observed</span>, <span class="va">p_true</span><span class="op">)</span><span class="op">)</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="fu"><a href="../reference/scoring-functions-binary.html">logs_binary</a></span><span class="op">(</span><span class="va">observed</span>, <span class="va">p_under</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.04741833</span></span></code></pre></div>
<p>See <code>?logs_binary()</code> for more information.</p>
<hr>
</div>
</div>
<div class="section level2">
<h2 id="sample-based-forecasts">Sample-based forecasts<a class="anchor" aria-label="anchor" href="#sample-based-forecasts"></a>
</h2>
<p>See a list of the default metrics for sample-based forecasts by
calling <code>?metrics_sample()</code>.</p>
<p>This is an overview of the input and output formats for quantile
forecasts:</p>
<div class="figure">
<img src="scoring-rules/input-sample.png" alt="Input and output formats: metrics for sample-based forecasts." width="100%"><p class="caption">
Input and output formats: metrics for sample-based forecasts.
</p>
</div>
<div class="section level3">
<h3 id="crps">CRPS<a class="anchor" aria-label="anchor" href="#crps"></a>
</h3>
<p><strong>Observation</strong>: <span class="math inline">\(y\)</span>,
a real number (or a discrete number).</p>
<p><strong>Forecast</strong>: A continuous (<span class="math inline">\(F\)</span>) or discrete (<span class="math inline">\(P\)</span>) forecast.</p>
<p>The continuous ranked probability score (CRPS) is popular in fields
such as meteorology and epidemiology. The CRPS is defined as <span class="math display">\[\text{CRPS}(F, y) = \int_{-\infty}^\infty \left(
F(x) - 1(x \geq y) \right)^2 dx,\]</span> where <span class="math inline">\(y\)</span> is the observed value and <span class="math inline">\(F\)</span> the CDF of predictive distribution.</p>
<p>For discrete forecasts, for example count data, the ranked
probability score (RPS) can be used instead and is commonly defined as:
<span class="math display">\[ \text{RPS}(P, y) = \sum_{x = 0}^\infty
(P(x) - 1(x \geq y))^2, \]</span> where <span class="math inline">\(P\)</span> is the cumulative probability mass
function (PMF) of the predictive distribution.</p>
<p>The CRPS can be understood as a generalisation of the absolute error
to predictive distributions <span class="citation">(Gneiting and Raftery
2007)</span>. It can also be understood as the integral over the Brier
score for the binary probability forecasts implied by the CDF for all
possible observed values. The CRPS is also related to the
Cramér-distance between two distributions and equals the special case
where one of the distributions is concentrated in a single point (see
e.g. <span class="citation">Ziel (2021)</span>). The CRPS is a global
scoring rule, meaning that the entire predictive distribution is taken
into account when determining the quality of the forecast.</p>
<p><code>scoringutils</code> re-exports the <code><a href="../reference/crps_sample.html">crps_sample()</a></code>
function from the <code>scoringRules</code> package, which assumes that
the forecast is represented by a set of samples from the predictive
distribution. See <code>?crps_sample()</code> for more information.</p>
</div>
<div class="section level3">
<h3 id="log-score">Log score<a class="anchor" aria-label="anchor" href="#log-score"></a>
</h3>
<p><strong>Observation</strong>: <span class="math inline">\(y\)</span>,
a real number (or a discrete number).</p>
<p><strong>Forecast</strong>: A continuous (<span class="math inline">\(F\)</span>) or discrete (<span class="math inline">\(P\)</span>) forecast.</p>
<p>The logarithmic scoring rule is simply the negative logarithm of the
density of the the predictive distribution evaluated at the observed
value:</p>
<p><span class="math display">\[ \text{log score}(F, y) = -\log f(y),
\]</span></p>
<p>where <span class="math inline">\(f\)</span> is the predictive
probability density function (PDF) corresponding to the Forecast <span class="math inline">\(F\)</span> and <span class="math inline">\(y\)</span> is the observed value.</p>
<p>For discrete forecasts, the log score can be computed as</p>
<p><span class="math display">\[ \text{log score}(F, y) = -\log p_y,
\]</span> where <span class="math inline">\(p_y\)</span> is the
probability assigned to the observed outcome <span class="math inline">\(y\)</span> by the forecast <span class="math inline">\(F\)</span>.</p>
<p>The logarithmic scoring rule can produce large penalties when the
observed value takes on values for which <span class="math inline">\(f(y)\)</span> (or <span class="math inline">\(p_y\)</span>) is close to zero. It is therefore
considered to be sensitive to outlier forecasts. This may be desirable
in some applications, but it also means that scores can easily be
dominated by a few extreme values. The logarithmic scoring rule is a
local scoring rule, meaning that the score only depends on the
probability that was assigned to the actual outcome. This is often
regarded as a desirable property for example in the context of Bayesian
inference . It implies for example, that the ranking between forecasters
would be invariant under monotone transformations of the predictive
distribution and the target.</p>
<p><code>scoringutils</code> re-exports the <code><a href="../reference/logs_sample.html">logs_sample()</a></code>
function from the <code>scoringRules</code> package, which assumes that
the forecast is represented by a set of samples from the predictive
distribution. One implications of this is that it is currently not
advisable to use the log score for discrete forecasts. The reason for
this is that <code><a href="https://rdrr.io/pkg/scoringRules/man/scores_sample_univ.html" class="external-link">scoringRules::logs_sample()</a></code> estimates a
predictive density from the samples, which can be problematic for
discrete forecasts.</p>
<p>See <code>?logs_sample()</code> for more information.</p>
</div>
<div class="section level3">
<h3 id="dawid-sebastiani-score">Dawid-Sebastiani score<a class="anchor" aria-label="anchor" href="#dawid-sebastiani-score"></a>
</h3>
<p><strong>Observation</strong>: <span class="math inline">\(y\)</span>,
a real number (or a discrete number).</p>
<p><strong>Forecast</strong>: <span class="math inline">\(F\)</span>.
The predictive distribution with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>.</p>
<p>The Dawid-Sebastiani score is a proper scoring rule that only relies
on the first moments of the predictive distribution and is therefore
easy to compute. It is given as</p>
<p><span class="math display">\[\text{dss}(F, y) = \left( \frac{y -
\mu}{\sigma} \right)^2 + 2 \cdot \log \sigma.\]</span></p>
<p><code>scoringutils</code> re-exports the implementation of the DSS
from the <code>scoringRules</code> package. It assumes that the forecast
is represented by a set of samples drawn from the predictive
distribution. See <code>?dss_sample()</code> for more information.</p>
</div>
<div class="section level3">
<h3 id="dispersion---median-absolute-deviation-mad">Dispersion - Median Absolute Deviation (MAD)<a class="anchor" aria-label="anchor" href="#dispersion---median-absolute-deviation-mad"></a>
</h3>
<p><strong>Observation</strong>: Not required.</p>
<p><strong>Forecast</strong>: <span class="math inline">\(F\)</span>,
the predictive distribution.</p>
<p>Dispersion (also called sharpness) is the ability to produce narrow
forecasts. It is a feature of the forecasts only and does not depend on
the observations. Dispersion is therefore only of interest conditional
on calibration: a very precise forecast is not useful if it is clearly
wrong.</p>
<p>One way to measure sharpness (as suggested by <span class="citation">Funk et al. (2019)</span>) is the normalised median
absolute deviation about the median (MADN) ). It is computed as <span class="math display">\[ S(F) = \frac{1}{0.675} \cdot \text{median}(|F -
\text{median(F)}|). \]</span> If the forecast <span class="math inline">\(F\)</span> follows a normal distribution, then
sharpness will equal the standard deviation of <span class="math inline">\(F\)</span>.For more details, see
<code>?mad_sample()</code>.</p>
</div>
<div class="section level3">
<h3 id="bias">Bias<a class="anchor" aria-label="anchor" href="#bias"></a>
</h3>
<p><strong>Observation</strong>: <span class="math inline">\(y\)</span>,
a real number (or a discrete number).</p>
<p><strong>Forecast</strong>: A continuous (<span class="math inline">\(F\)</span>) or discrete (<span class="math inline">\(P\)</span>) forecast.</p>
<p>Bias is a measure of the tendency of a forecaster to over- or
underpredict. For <em>continuous</em> forecasts, the
<code>scoringutils</code> implementation calculates bias as <span class="math display">\[B(F, y) = 1 - 2 \cdot F (y), \]</span> where
<span class="math inline">\(F(y)\)</span> is the cumulative distribution
function of the forecast.</p>
<p>For <em>discrete</em> forecasts, we calculate bias as <span class="math display">\[B(P, y) = 1 - (P(y) + P(y + 1)). \]</span> where
<span class="math inline">\(P(y)\)</span> is the cumulative probability
assigned to all outcomes smaller or equal to <span class="math inline">\(y\)</span>, i.e. the cumulative probability mass
function.</p>
<p>Bias is bound between -1 and 1 and represents the tendency of
forecasts to be biased rather than the absolute amount of over- and
underprediction (which is e.g. the case for the weighted interval score
(WIS, see below).</p>
</div>
<div class="section level3">
<h3 id="absolute-error-of-the-median">Absolute error of the median<a class="anchor" aria-label="anchor" href="#absolute-error-of-the-median"></a>
</h3>
<p><strong>Observation</strong>: <span class="math inline">\(y\)</span>,
a real number (or a discrete number).</p>
<p><strong>Forecast</strong>: A forecast <span class="math inline">\(F\)</span></p>
<p><span class="math display">\[\text{ae}_{\text{median}}(F, y) =
|\text{median} (F) - y|.\]</span> See section <a href="#a-note-of-caution">A note of caution</a> or <span class="citation">Gneiting (2011)</span> for a discussion on the
correspondence between the absolute error and the median.</p>
</div>
<div class="section level3">
<h3 id="squared-error-of-the-mean">Squared error of the mean<a class="anchor" aria-label="anchor" href="#squared-error-of-the-mean"></a>
</h3>
<p><strong>Observation</strong>: <span class="math inline">\(y\)</span>,
a real number (or a discrete number).</p>
<p><strong>Forecast</strong>: A forecast <span class="math inline">\(F\)</span></p>
<p><span class="math display">\[\text{se}_{\text{medn}}(F, y) =
(\text{mean} (F) - y)^2.\]</span> See section <a href="#a-note-of-caution">A note of caution</a> or <span class="citation">Gneiting (2011)</span> for a discussion on the
correspondence between the squared error and the mean.</p>
<hr>
</div>
</div>
<div class="section level2">
<h2 id="quantile-based-forecasts">Quantile-based forecasts<a class="anchor" aria-label="anchor" href="#quantile-based-forecasts"></a>
</h2>
<p>See a list of the default metrics for quantile-based forecasts by
calling <code>?metrics_quantile()</code>.</p>
<p>This is an overview of the input and output formats for quantile
forecasts:</p>
<div class="figure">
<img src="scoring-rules/input-quantile.png" alt="Input and output formats: metrics for quantile-based forecasts." width="100%"><p class="caption">
Input and output formats: metrics for quantile-based forecasts.
</p>
</div>
<div class="section level3">
<h3 id="weighted-interval-score-wis">Weighted interval score (WIS)<a class="anchor" aria-label="anchor" href="#weighted-interval-score-wis"></a>
</h3>
<p><strong>Observation</strong>: <span class="math inline">\(y\)</span>,
a real number</p>
<p><strong>Forecast</strong>: <span class="math inline">\(F\)</span>.
The CDF of the predictive distribution is represented by a set of
quantiles. These quantiles form the lower (<span class="math inline">\(l\)</span>) and upper (<span class="math inline">\(u\)</span>) bounds of central prediction
intervals.</p>
<p>The weighted interval score (WIS) is a strictly proper scoring rule
and can be understood as an approximation of the CRPS for forecasts in a
quantile format (which in turn represents a generalisation of the
absolute error). Quantiles are assumed to be the lower and upper bounds
of prediction intervals symmetric around the median. For a single
interval, the interval score is</p>
<p><span class="math display">\[IS_\alpha(F,y) =
\underbrace{(u-l)}_\text{dispersion} + \underbrace{\frac{2}{\alpha}
\cdot (l-y) \cdot \mathbf{1}(y \leq l)}_{\text{overprediction}} +
\underbrace{\frac{2}{\alpha} \cdot (y-u) \cdot \mathbf{1}(y \geq
u)}_{\text{underprediction}}, \]</span></p>
<p>where <span class="math inline">\(\mathbf{1}()\)</span> is the
indicator function, and <span class="math inline">\(l\)</span> and <span class="math inline">\(u\)</span> are the <span class="math inline">\(\frac{\alpha}{2}\)</span> and <span class="math inline">\(1 - \frac{\alpha}{2}\)</span> quantiles of the
predictive distribution <span class="math inline">\(F\)</span>. <span class="math inline">\(l\)</span> and <span class="math inline">\(u\)</span> together form the prediction interval.
The interval score can be understood as the sum of three components:
dispersion, overprediction and underprediction.</p>
<p>For a set of <span class="math inline">\(K\)</span> prediction
intervals and the median <span class="math inline">\(m\)</span>, the
score is given as a weighted sum of individual interval scores, i.e.</p>
<p><span class="math display">\[WIS = \frac{1}{K + 0.5} \cdot \left(w_0
\cdot |y - m| + \sum_{k = 1}^{K} w_k \cdot IS_{\alpha_{k}}(F,
y)\right),\]</span> where <span class="math inline">\(m\)</span> is the
median forecast and <span class="math inline">\(w_k\)</span> is a weight
assigned to every interval. When the weights are set to <span class="math inline">\(w_k = \frac{\alpha_k}{2}\)</span> and <span class="math inline">\(w_0 = 0.5\)</span>, then the WIS converges to the
CRPS for an increasing number of equally spaced quantiles.</p>
<p>See <code>?wis()</code> for more information.</p>
<div class="section level4">
<h4 id="overprediction-underprediction-and-dispersion">Overprediction, underprediction and dispersion<a class="anchor" aria-label="anchor" href="#overprediction-underprediction-and-dispersion"></a>
</h4>
<p>These are the individual components of the WIS. See
<code>?overprediction()</code>, <code>?underprediction()</code> and
<code>?dispersion()</code> for more information.</p>
</div>
</div>
<div class="section level3">
<h3 id="bias-1">Bias<a class="anchor" aria-label="anchor" href="#bias-1"></a>
</h3>
<p><strong>Observation</strong>: <span class="math inline">\(y\)</span>,
a real number</p>
<p><strong>Forecast</strong>: <span class="math inline">\(F\)</span>.
The CDF of the predictive distribution is represented by a set of
quantiles, <span class="math inline">\(Q\)</span>.</p>
<p>Bias can be measured as</p>
<p><span class="math display">\[\begin{equation}
    \text{B}(F, y) =
    \begin{cases}
        (1 - 2 \cdot \max \{\alpha | q_\alpha \in Q \land q_\alpha \leq
y\}),       &amp; \text{if } y &lt; q_{0.5} \quad
\text{(overprediction)}\\
        (1 - 2 \cdot \min \{\alpha | q_\alpha \in Q_t \land q_\alpha
\geq y\},   &amp; \text{if } y &gt; q_{0.5} \quad
\text{(underprediction)}\\
        0,   &amp; \text{if } y = q_{0.5}, \\
    \end{cases}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(q_\alpha\)</span> is the <span class="math inline">\(\alpha\)</span>-quantile of the predictive
distribution. For consistency, we define <span class="math inline">\(Q\)</span> (the set of quantiles that form the
predictive distribution <span class="math inline">\(F\)</span>) such
that it always includes the element <span class="math inline">\(q_0 =
-\infty\)</span> and <span class="math inline">\(q_1 = \infty\)</span>.
In clearer terms, bias is:</p>
<ul>
<li>
<span class="math inline">\(1 - (2 \times\)</span> the maximum
percentile rank for which the corresponding quantile is still below the
observed value), <em>if the observed value is smaller than the median of
the predictive distribution.</em>
</li>
<li>
<span class="math inline">\(1 - (2 \times\)</span> the minimum
percentile rank for which the corresponding quantile is still larger
than the observed value) <em>if the observed value is larger than the
median of the predictive distribution.</em>.</li>
<li>
<span class="math inline">\(0\)</span> <em>if the observed value is
exactly the median</em>.</li>
</ul>
<p>Bias can assume values between -1 (underprediction) and 1
(overpredictin) and is 0 ideally (i.e. unbiased).</p>
<p>For an increasing number of quantiles, the percentile rank will equal
the proportion of predictive samples below the observed value, and the
bias metric coincides with the one for continuous forecasts (see
above).</p>
<p>See <code>?bias_quantile()</code> for more information.</p>
</div>
<div class="section level3">
<h3 id="interval-coverage">Interval coverage<a class="anchor" aria-label="anchor" href="#interval-coverage"></a>
</h3>
<p><strong>Observation</strong>: <span class="math inline">\(y\)</span>,
a real number</p>
<p><strong>Forecast</strong>: <span class="math inline">\(F\)</span>.
The CDF of the predictive distribution is represented by a set of
quantiles. These quantiles form central prediction intervals.</p>
<p>Interval coverage for a given interval range is defined as the
proportion of observations that fall within the corresponding central
prediction intervals. Central prediction intervals are symmetric around
the median and formed by two quantiles that denote the lower and upper
bound. For example, the 50% central prediction interval is the interval
between the 0.25 and 0.75 quantiles of the predictive distribution.</p>
<div class="section level4">
<h4 id="interval-coverage-deviation">Interval coverage deviation<a class="anchor" aria-label="anchor" href="#interval-coverage-deviation"></a>
</h4>
<p>The interval coverage deviation is the difference between the
observed interval coverage and the nominal interval coverage. For
example, if the observed interval coverage for the 50% central
prediction interval is 0.6, then the interval coverage deviation is
<span class="math inline">\(0.6 = - 0.5 = 0.1.\)</span></p>
<p><span class="math display">\[\text{interval coverage deviation} =
\text{observed interval coverage} - \text{nominal interval
coverage}\]</span></p>
</div>
</div>
<div class="section level3">
<h3 id="absolute-error-of-the-median-1">Absolute error of the median<a class="anchor" aria-label="anchor" href="#absolute-error-of-the-median-1"></a>
</h3>
<p><strong>Observation</strong>: <span class="math inline">\(y\)</span>,
a real number</p>
<p><strong>Forecast</strong>: <span class="math inline">\(F\)</span>.
The CDF of the predictive distribution is represented by a set of
quantiles.</p>
<p>The absolute error of the median is the absolute difference between
the median of the predictive distribution and the observed value.</p>
<p><span class="math display">\[\text{ae}_\text{median} =
|\text{median}(F) - y|\]</span> See section <a href="#a-note-of-caution">A note of caution</a> or <span class="citation">Gneiting (2011)</span> for a discussion on the
correspondence between the absolute error and the median.</p>
</div>
<div class="section level3">
<h3 id="quantile-score">Quantile score<a class="anchor" aria-label="anchor" href="#quantile-score"></a>
</h3>
<p><strong>Observation</strong>: <span class="math inline">\(y\)</span>,
a real number</p>
<p><strong>Forecast</strong>: <span class="math inline">\(F\)</span>.
The CDF of the predictive distribution is represented by a set of
quantiles.</p>
<p>The quantile score, also called pinball loss, for a single quantile
level <span class="math inline">\(\tau\)</span> is defined as</p>
<p><span class="math display">\[\begin{equation}
    \text{QS}_\tau(F, y) = 2 \cdot \{ \mathbf{1}(y \leq q_\tau) - \tau\}
\cdot (q_\tau − y) =
    \begin{cases}
        2 \cdot (1 - \tau) * q_\tau - y,       &amp; \text{if } y \leq
q_\tau\\
        2 \cdot \tau * |q_\tau - y|,           &amp; \text{if } y &gt;
q_\tau,
    \end{cases}
\end{equation}\]</span></p>
<p>with <span class="math inline">\(q_\tau\)</span> being the <span class="math inline">\(\tau\)</span>-quantile of the predictive
distribution <span class="math inline">\(F\)</span>, and <span class="math inline">\(\mathbf{1}(\cdot)\)</span> the indicator
function.</p>
<p>The (unweighted) interval score (see above) for a <span class="math inline">\(1 - \alpha\)</span> prediction interval can be
computed from the quantile scores at levels <span class="math inline">\(\alpha/2\)</span> and <span class="math inline">\(1 - \alpha/2\)</span> as</p>
<p><span class="math display">\[\text{IS}_\alpha(F, y) =
\frac{\text{QS}_{\alpha/2}(F, y) + \text{QS}_{1 - \alpha/2}(F,
y)}{\alpha}\]</span>.</p>
<p>The weighted interval score can be obtained as a simple average of
the quantile scores:</p>
<p><span class="math display">\[\text{WIS}_\alpha(F, y) =
\frac{\text{QS}_{\alpha/2}(F, y) + \text{QS}_{1 - \alpha/2}(F,
y)}{2}\]</span>.</p>
<p>See <code><a href="../reference/quantile_score.html">?quantile_score</a></code> and <span class="citation">Bracher
et al. (2021)</span> for more details.</p>
<hr>
</div>
</div>
<div class="section level2">
<h2 id="additional-metrics">Additional metrics<a class="anchor" aria-label="anchor" href="#additional-metrics"></a>
</h2>
<div class="section level3">
<h3 id="quantile-coverage">Quantile coverage<a class="anchor" aria-label="anchor" href="#quantile-coverage"></a>
</h3>
<p>Quantile coverage for a given quantile level is defined as the
proportion of observed values that are smaller than the corresponding
predictive quantiles. For example, the 0.5 quantile coverage is the
proportion of observed values that are smaller than the 0.5-quantiles of
the predictive distribution.</p>
<!-- ## Probability integral transform (PIT) -->
<!-- # Calibration -->
<!-- Calibration or reliability of forecasts is the ability of a model to -->
<!-- correctly identify its own uncertainty in making predictions. In a model -->
<!-- with perfect calibration, the observed data at each time point look as if -->
<!-- they came from the predictive probability distribution at that time. -->
<!-- Equivalently, one can inspect the probability integral transform of the -->
<!-- predictive distribution at time t, -->
<!-- $$u_t = F_t (x_t)$$ -->
<!-- where $x_t$ is the observed data point at time $t \text{ in } t_1, …, t_n$, -->
<!-- n being the number of forecasts, and $F_t$ is the (continuous) predictive -->
<!-- cumulative probability distribution at time t. If the true probability -->
<!-- distribution of outcomes at time t is $G_t$ then the forecasts $F_t$ are -->
<!-- said to be ideal if $F_t = G_t$ at all times $t$. In that case, the -->
<!-- probabilities ut are distributed uniformly. -->
<!-- In the case of discrete outcomes such as incidence counts, -->
<!-- the PIT is no longer uniform even when forecasts are ideal. -->
<!-- In that case a randomised PIT can be used instead: -->
<!-- $$u_t = P_t(k_t) + v \cdot (P_t(k_t) - P_t(k_t - 1) )$$ -->
<!-- where $k_t$ is the observed count, $P_t(x)$ is the predictive -->
<!-- cumulative probability of observing incidence $k$ at time $t$, -->
<!-- $P_t (-1) = 0$ by definition and $v$ is standard uniform and independent -->
<!-- of $k$. If $P_t$ is the true cumulative -->
<!-- probability distribution, then $u_t$ is standard uniform. -->
<!-- The function checks whether integer or continuous forecasts were provided. -->
<!-- It then applies the (randomised) probability integral and tests -->
<!-- the values $u_t$ for uniformity using the -->
<!-- Anderson-Darling test. -->
<!-- As a rule of thumb, there is no evidence to suggest a forecasting model is -->
<!-- miscalibrated if the p-value found was greater than a threshold of $p >= 0.1$, -->
<!-- some evidence that it was miscalibrated if $0.01 < p < 0.1$, and good -->
<!-- evidence that it was miscalibrated if $p <= 0.01$. -->
<!-- In this context it should be noted, though, that uniformity of the -->
<!-- PIT is a necessary but not sufficient condition of calibration. It should -->
<!-- also be noted that the test only works given sufficient samples, otherwise the  -->
<!-- Null hypothesis will often be rejected outright.  -->
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-bracherEvaluatingEpidemicForecasts2021" class="csl-entry">
Bracher, Johannes, Evan L. Ray, Tilmann Gneiting, and Nicholas G. Reich.
2021. <span>“Evaluating Epidemic Forecasts in an Interval
Format.”</span> <em>PLoS Computational Biology</em> 17 (2): e1008618. <a href="https://doi.org/10.1371/journal.pcbi.1008618" class="external-link">https://doi.org/10.1371/journal.pcbi.1008618</a>.
</div>
<div id="ref-funkAssessingPerformanceRealtime2019" class="csl-entry">
Funk, Sebastian, Anton Camacho, Adam J. Kucharski, Rachel Lowe, Rosalind
M. Eggo, and W. John Edmunds. 2019. <span>“Assessing the Performance of
Real-Time Epidemic Forecasts: <span>A</span> Case Study of
<span>Ebola</span> in the <span>Western Area</span> Region of
<span>Sierra Leone</span>, 2014-15.”</span> <em>PLOS Computational
Biology</em> 15 (2): e1006785. <a href="https://doi.org/10.1371/journal.pcbi.1006785" class="external-link">https://doi.org/10.1371/journal.pcbi.1006785</a>.
</div>
<div id="ref-gneitingMakingEvaluatingPoint2011" class="csl-entry">
Gneiting, Tilmann. 2011. <span>“Making and <span>Evaluating Point
Forecasts</span>.”</span> <em>Journal of the American Statistical
Association</em> 106 (494): 746–62. <a href="https://doi.org/10.1198/jasa.2011.r10138" class="external-link">https://doi.org/10.1198/jasa.2011.r10138</a>.
</div>
<div id="ref-gneitingStrictlyProperScoring2007" class="csl-entry">
Gneiting, Tilmann, and Adrian E Raftery. 2007. <span>“Strictly
<span>Proper Scoring Rules</span>, <span>Prediction</span>, and
<span>Estimation</span>.”</span> <em>Journal of the American Statistical
Association</em> 102 (477): 359–78. <a href="https://doi.org/10.1198/016214506000001437" class="external-link">https://doi.org/10.1198/016214506000001437</a>.
</div>
<div id="ref-macheteContrastingProbabilisticScoring2012" class="csl-entry">
Machete, Reason Lesego. 2012. <span>“Contrasting <span>Probabilistic
Scoring Rules</span>.”</span> <em>arXiv:1112.4530 [Math, Stat]</em>,
July. <a href="https://arxiv.org/abs/1112.4530" class="external-link">https://arxiv.org/abs/1112.4530</a>.
</div>
<div id="ref-zielEnergyDistanceEnsemble2021" class="csl-entry">
Ziel, Florian. 2021. <span>“The Energy Distance for Ensemble and
Scenario Reduction.”</span> <em>Philosophical Transactions of the Royal
Society A: Mathematical, Physical and Engineering Sciences</em> 379
(2202): 20190431. <a href="https://doi.org/10.1098/rsta.2019.0431" class="external-link">https://doi.org/10.1098/rsta.2019.0431</a>.
</div>
</div>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by <a href="https://followtheargument.org/" class="external-link">Nikos Bosse</a>, <a href="https://www.samabbott.co.uk/" class="external-link">Sam Abbott</a>, Hugo Gruson, Sebastian Funk.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.8.9000.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
