% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/summarise_scores.R
\name{summarise_scores}
\alias{summarise_scores}
\title{Summarise scores as produced by \code{\link[=score]{score()}}}
\usage{
summarise_scores(
  scores,
  by = NULL,
  func = mean,
  relative_skill = FALSE,
  metric = "auto",
  baseline = NULL,
  ...
)
}
\arguments{
\item{scores}{A data.table of scores as produced by \code{\link[=score]{score()}}.}

\item{by}{character vector with column names to summarise scores by. Default
is \code{NULL}, meaning that the only summary that takes is place is summarising
over quantiles (in case of quantile-based forecasts), such that there is one
score per forecast as defined by the unit of a single forecast (rather than
one score for every quantile).}

\item{func}{a function used for summarising scores. Default is \code{mean}.}

\item{relative_skill}{logical, whether or not to compute relative
performance between models based on pairwise comparisons.
If \code{TRUE} (default is \code{FALSE}), then a column called
'model' must be present in the input data. For more information on
the computation of relative skill, see \code{\link[=pairwise_comparison]{pairwise_comparison()}}.
Relative skill will be calculated for the aggregation level specified in
\code{by}.}

\item{metric}{character with the name of the metric for which
a relative skill shall be computed. If equal to 'auto' (the default), then
this will be either interval score, crps or brier score (depending on which
of these is available in the input data)}

\item{baseline}{character string with the name of a model. If a baseline is
given, then a scaled relative skill with respect to the baseline will be
returned. By default (\code{NULL}), relative skill will not be scaled with
respect to a baseline model.}

\item{...}{additional arguments, such as test options that can get passed
down to lower level functions. The following options are available:
\code{one_sided} (Boolean, default is \code{FALSE}, whether two conduct a one-sided
instead of a two-sided test), \code{test_type} (character, either "non_parametric"
or "permutation" determining which kind of test shall be conducted to
determine p-values. Default is "non-parametric), \code{n_permutations} (number of
permutations for a permutation test. Default is 999). See
\code{\link[=compare_two_models]{compare_two_models()}} for more information.}
}
\description{
Summarise scores as produced by \code{\link[=score]{score()}}-
}
\examples{
library(magrittr) # pipe operator

# summarise over samples or quantiles to get one score per forecast
scores <- score(example_quantile)
summarise_scores(scores)

# get scores by model
summarise_scores(scores, by = c("model"))

# get scores by model and target type
summarise_scores(scores, by = c("model", "target_type"))

# get standard deviation
summarise_scores(scores, by = "model", func = sd)

# get quantiles of scores
# make sure to aggregate over ranges first
summarise_scores(scores,
  by = "model", func = quantile,
  probs = c(0.25, 0.5, 0.75)
)

# get ranges
# summarise_scores(scores, by = "range")
}
\keyword{scoring}
