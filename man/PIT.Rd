% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/fun_probabilstic_forecasts_integer.R
\name{PIT}
\alias{PIT}
\title{randomized Probability Integral Transformation}
\usage{
PIT(true_values, samples, num_bins = NULL, n_replicates = 20)
}
\arguments{
\item{true_values}{A vector with the true observed values of size n}

\item{samples}{nxN matrix of predictive samples, n (number of rows) being
the number of data points and N (number of columns) the
number of Monte Carlo samples}

\item{num_bins}{the number of bins in the PIT histogram.
If not given, the square root of n will be used}

\item{n_replicates}{the number of tests to perform,
each time re-randomising the PIT}
}
\value{
a list with the following components:
\itemize{
\item \code{p_values}: p-values from the
Anderson-Darling test on the \code{n_replicate} replicates of the randomised
PIT
\item \code{hist_PIT} a ggplot object with the PIT histogram. Call
\code{plot(PIT(...)$hist_PIT)} to display the histogram.
\item \code{calibration}: mean and standard deviation of the p-values of the
\code{n_replicates} Anderson-Darling tests. This can be used as a summary
to assess calibration of the predictions.
}
}
\description{
Uses a (randomised) Probability Integral Transformation (PIT) to
assess the calibration of predictive Monte Carlo samples. Returns a
randomised PIT histogram and p-values resulting from an Anderson-Darling
test for uniformity for the randomised PIT.
}
\details{
Calibration or reliability of forecasts is the ability of a model to
correctly identify its own uncertainty in making predictions. In a model
with perfect calibration, the observed data at each time point look as if
they came from the predictive probability distribution at that time.

Equivalently, one can inspect the probability integral transform of the
predictive distribution at time t,

\deqn{
u_t = F_t (x_t)
}

where \eqn{x_t} is the observed data point at time \eqn{t in t_1, …, t_n},
n being the number of forecasts, and $F_t$ is the (continuous) predictive
cumulative probability distribution at time t. If the true probability
distribution of outcomes at time t is \eqn{G_t} then the forecasts eqn{F_t} are
said to be ideal if eqn{F_t = G_t} at all times t. In that case, the
probabilities ut are distributed uniformly.

In the case of discrete outcomes such as the incidence counts that were
forecast here, the PIT is no longer uniform even when forecasts are ideal.
In that case a randomised PIT can be used instead:
\deqn{
u_t = P_t(k_t) + v * (P_t(k_t) - P_t(k_t - 1) )
}

where \eqn{k_t} is the observed count, \eqn{P_t(x)} is the predictive
cumulative probability of observing incidence k at time t,
eqn{P_t (−1) = 0} by definition and v is standard uniform and independent
of k. If \eqn{P_t} is the true cumulative
probability distribution, then \eqn{u_t} is standard uniform.

The resulting values \eqn{u_t} are then tested for uniformity using the
Anderson-Darling test.

As a rule of thumb, there is no evidence to suggest a forecasting model is
miscalibrated if the p-value found was greater than a threshold of p ≥ 0.1,
some evidence that it was miscalibrated if 0.01 < p < 0.1, and good
evidence that it was miscalibrated if p ≤ 0.01.
In this context it should be noted, though, that uniformity of the
(randomised) PIT is a necessary but not sufficient condition of calibration.
}
\examples{
NULL
}
\references{
Funk S, Camacho A, Kucharski AJ, Lowe R, Eggo RM, Edmunds WJ (2019)
Assessing the performance of real-time epidemic forecasts: A case study of
Ebola in the Western Area region of Sierra Leone, 2014-15.
PLoS Comput Biol 15(2): e1006785.
\url{https://doi.org/10.1371/journal.pcbi.1006785}

Gneiting, T., Balabdaoui, F. and Raftery, A.E. (2007), Probabilistic
forecasts, calibration and sharpness. Journal of the Royal Statistical
Society: Series B (Statistical Methodology), 69: 243-268.
\url{https://doi.org/10.1111/j.1467-9868.2007.00587.x}

Dawid, A. (1984). Present Position and Potential Developments: Some Personal
Views: Statistical Theory: The Prequential Approach. Journal of the Royal
Statistical Society. Series A (General), 147(2), 278-292.
doi:10.2307/2981683
}
\seealso{
Czado, C., Gneiting, T. and Held, L. (2009),
Predictive Model Assessment for Count Data. Biometrics, 65:
1254-1261. \url{doi:10.1111/j.1541-0420.2009.01191.x}
}
