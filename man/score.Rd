% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/score.R
\name{score}
\alias{score}
\title{Evaluate forecasts}
\usage{
score(data, summarise_by = NULL, metrics = NULL, ...)
}
\arguments{
\item{data}{A data.frame or data.table with the predictions and observations.
Note: it is easiest to have a look at the example files provided in the
package and in the examples below.
The following columns need to be present:
\itemize{
\item \code{true_value} - the true observed values
\item \code{prediction} - predictions or predictive samples for one
true value. (You only don't need to provide a prediction column if
you want to score quantile forecasts in a wide range format.)}
For integer and continuous forecasts a \code{sample} column is needed:
\itemize{
\item \code{sample} - an index to identify the predictive samples in the
prediction column generated by one model for one true value. Only
necessary for continuous and integer forecasts, not for
binary predictions.}
For a quantile-format forecast you should provide:
\itemize{
\item \code{prediction}: prediction to the corresponding quantile
\item \code{quantile}: quantile to which the prediction corresponds
}}

\item{summarise_by}{character vector of columns to group the summary by. By
default, this is equal to \code{by} and no summary takes place.
But sometimes you may want to to summarise
over categories different from the scoring.
\code{summarise_by} is also the grouping level used to compute
(and possibly plot) the probability integral transform(pit). Sometimes you
may want to include 'range', 'quantile' or 'sample', to summarise by
range, quantile or sample.}

\item{metrics}{the metrics you want to have in the output. If \code{NULL} (the
default), all available metrics will be computed. For a list of available
metrics see \code{\link[=available_metrics]{available_metrics()}}}

\item{...}{additional parameters passed down to lower-level functions.
For example, the following arguments can change how weighted interval
scores are computed:
\itemize{
\item \code{count_median_twice} that controls how the interval scores for different
intervals are summed up. This should be a logical (default is \code{FALSE}) that
indicates whether or not to count the median twice when summarising.
This would conceptually treat the
median as a 0\% prediction interval, where the median is the lower as well as
the upper bound. The alternative is to treat the median as a single quantile
forecast instead of an interval. The interval score would then
be better understood as an average of quantile scores.)
}}
}
\value{
A data.table with appropriate scores. For binary predictions,
the Brier Score will be returned, for quantile predictions the interval
score, as well as adapted metrics for calibration, sharpness/dispersion and
bias. For integer forecasts, Sharpness, Bias, DSS, CRPS, LogS, and
pit_p_val (as an indicator of calibration) are returned. For integer
forecasts, pit_sd is returned (to account for the randomised PIT),
but no Log Score is returned (the internal estimation relies on a
kernel density estimate which is difficult for integer-valued forecasts).
If \code{summarise_by} is specified differently from \code{by},
the average score per summary unit is returned.
If specified, quantiles and standard deviation of scores can also be returned
when summarising.
}
\description{
The function \code{score} is an easy to use wrapper
of the lower level functions in the \pkg{scoringutils} package.
It can be used to score probabilistic or quantile forecasts of
continuous, integer-valued or binary variables.
}
\details{
the following metrics are used where appropriate:
\itemize{
\item {Interval Score} for quantile forecasts. Smaller is better. See
\code{\link[=interval_score]{interval_score()}} for more information. By default, the
weighted interval score is used.
\item {Brier Score} for a probability forecast of a binary outcome.
Smaller is better. See \code{\link[=brier_score]{brier_score()}} for more information.
\item {aem} Absolute error of the median prediction
\item {Bias} 0 is good, 1 and -1 are bad.
See \code{\link[=bias]{bias()}} for more information.
\item {Sharpness/dispersion} Smaller is better. See \code{\link[=sharpness]{sharpness()}} for more
information.
\item {Calibration} represented through the p-value of the
Anderson-Darling test for the uniformity of the Probability Integral
Transformation (PIT). For integer valued forecasts, this p-value also
has a standard deviation. Larger is better.
See \code{\link[=pit]{pit()}} for more information.
\item {DSS} Dawid-Sebastiani-Score. Smaller is better.
See \code{\link[=dss]{dss()}} for more information.
\item {CRPS} Continuous Ranked Probability Score. Smaller is better.
See \code{\link[=crps]{crps()}} for more information.
\item {Log Score} Smaller is better. Only for continuous forecasts.
See \code{\link[=logs]{logs()}} for more information.
}
}
\examples{
library("scoringutils")
## Probability Forecast for Binary Target
eval <- score(example_binary)

## Quantile Forecasts
eval <- score(example_quantile)

## Integer Forecasts
eval <- score(example_integer)

## Continuous Forecasts
eval <- score(example_continuous)

}
\references{
Funk S, Camacho A, Kucharski AJ, Lowe R, Eggo RM, Edmunds WJ
(2019) Assessing the performance of real-time epidemic forecasts: A
case study of Ebola in the Western Area region of Sierra Leone, 2014-15.
PLoS Comput Biol 15(2): e1006785. \url{doi:10.1371/journal.pcbi.1006785}
}
\author{
Nikos Bosse \email{nikosbosse@gmail.com}
}
